{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Berry Bengali: Blog Post 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alex Berry, Jason Chan, Hyunjoon Lee\n",
    "Brown University Data Science Initiative  \n",
    "DATA 2040: Deep Learning  \n",
    "March 3rd, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copy from 3/1 work journal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we took baseline model from the notebook (reference it)\n",
    "* list baseline model parameters here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tried keras hyperparameter tuner library, but was unable to do it, so we hard coded our own grid search with for loops basically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decided to try this set of parameters first\n",
    "\n",
    "activations = [\"tanh\", \"relu\"]\n",
    "\n",
    "dropout_probs = [0.2, 0.4]\n",
    "\n",
    "optimizers = ['adam', 'nadam']\n",
    "\n",
    "lr_schedulers = ['exp', 'power']\n",
    "\n",
    "each at epochs = 10\n",
    "\n",
    "early on we found that the exponential learning rate scheduler was performing very poorly, so we went with the power scheduler, which is original one fromt he baseline.\n",
    "\n",
    "Instead, we tried adding parameters\n",
    "\n",
    "batch_sizes = [256,128]\n",
    "\n",
    "Because we had intuition that smaller batch sizes in convolutional layers may perform better (google this, there's some articles on this).\n",
    "\n",
    "each model was taking 40 minutes to run, and in total with 16 models it would take about 10 hours to run. We trained models 0 through 5 (6 models), but even with 60GB of RAM on GCP, 16 CPUs, and 1 NVIDIA P100 GPU, it still ran out of memory. \n",
    "\n",
    "Analyzed these first five we’re noticing that Nadam isn’t performing well with these combinations. We also see that a lower dropout rate seems to give a better score on the validation set (not overfitting). We also see that batch size of 256 is performing better than batch size of 128. \n",
    "\n",
    "As a matter of time constraint we’ll remove nadam and batch size = 128 and continue with the other options. We decide to hold optimizer=Adam and batch_size=256. We already ran (tanh, 0.2, adam, 256) and (tanh, 0.4, adam, 256), so we wanted to compare it to (relu, 0.2, adam, 256) and (relu, 0.4, adam, 256), which is why we ran model 6 and 8.\n",
    "\n",
    "Ran those models and compare to baseline model. (you can just look at this table to compare and analyze). Mention the weighted average, based on the validation dense layer accuracy for root, vowel, and consonant, with double weight on root. We also kept track of the validation accuracy of the root dense layer, since thats the main one we care about.\n",
    "\n",
    "Pull in the image from /figures/model_tuning_results.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnn layer, mc dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final results from this tuning is that we realized ReLU is a better activation function than tanh, it seems the lower dropout probability leads to a slight increase in the root accuracy, the adam optimizer looks a lot better than nadam, and a batch size of 256 is better than 128.\n",
    "\n",
    "Therefore, we choose dropout = 0.2, we add one more convolutional module with filters=16, and run our final model for more 30 epochs.\n",
    "\n",
    "We add mc dropout in the prediction stage,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
