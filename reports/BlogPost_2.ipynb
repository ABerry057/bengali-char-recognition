{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Berry Bengali: Blog Post 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alex Berry, Jason Chan, Hyunjoon Lee\n",
    "Brown University Data Science Initiative  \n",
    "DATA 2040: Deep Learning  \n",
    "February 29th, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project involves classifying handwritten characters of the Bengali alphabet, similar to classifying integers in the MNIST data set. In particular, the Bengali alphabet is broken down into three components for each grapheme, or character: 1) the root, 2) the vowel diacritic, 3) the consonant diacritic, where a diacritic is similar to an accent. The goal is the create a classification model that can classify each of these three components of a handwritten grapheme, and the final result is measured using the recall metrics, with double weight given to classification of the root.\n",
    "\n",
    "Since our last blog post, we have focused our efforts on taking the given data and applying various preprocessing methods to both improve the performance of our model and decrease its training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivated by a desire for fine-tuned control over the input data's size and quality, we made the decision to apply preprocessing \"outside\" of the rest of the model pipeline. Our methodologies included cropping, resizing, denoising, and thresholding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cropping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first preprocessing method applied to the image data is cropping. Cropping was applied first because we believed it would be most appropriate to crop the images first and then to manipulate the pixels (denoising and thresholding) of the remaining bits of the images.\n",
    "\n",
    "Cropping is used to segment and maintain the most salient region (elements in region that stand out and attract the viewer's attention) of the image while cutting out the non-salient region. The position of the graphemes varies by image. Some graphemes are in the center, some are in top-left corner of the image. The size of the graphemes varies by image as well. Some graphemes fill the entire image, while some graphemes are slightly bigger than a dot. Our objective was to segment the graphemes and crop out the white space while maintaining as much information about the graphemes as possible.\n",
    "\n",
    "We implemented the function below to crop our images.\n",
    "\n",
    "```python\n",
    "def crop_surrounding_whitespace(image):\n",
    "    \"\"\"Remove surrounding empty space around an image.\n",
    "\n",
    "    This implemenation assumes that the surrounding empty space \n",
    "    around the image has the same colour as the top leftmost pixel.\n",
    "\n",
    "    :param image: PIL image\n",
    "    :rtype: PIL image (cropped)\n",
    "    \"\"\"\n",
    "    bg = Image.new(image.mode, image.size, image.getpixel((0,0)))\n",
    "    diff = ImageChops.difference(image, bg)\n",
    "    diff = ImageChops.add(diff, diff, 2, -50)\n",
    "    bbox = diff.getbbox()\n",
    "    return image.crop(bbox)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Resizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cropping, we resized the images so that every image has the same dimension. The images must be resized to the same dimension because each sample of CNN model's input must be of the same size. We determined the dimension of our resized images based on the distribution of the cropped images. Below is the distribution of row dimension (number of pixels) of the cropped images.\n",
    "\n",
    "![alt text](../figures/rows_crop.png)\n",
    "\n",
    "Below is the distribution of column dimension (number of pixels) of the cropped images.\n",
    "\n",
    "![alt text](../figures/cols_crop.png)\n",
    "\n",
    "The distribution of row dimension was slightly right-skewed and column dimension was symmetric. However, both distribution was bimodal with images that were not cropped (given that the orignal images were $137 \\times 236$). Therefore, we determined that resizing each row and column dimension by the median of their respective distribution the most appropriate. The resized dimension of the images were $106 \\times 87$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we resized the images, we converted the image from PIL Image to numpy array to use CV2 library tools. The first tool we used was `cv2.fastNlMeansDenoising()`. According to OpenCV docuemntation, this method performs image denoising using Non-Local Means Denoising algorithm (http://www.ipol.im/pub/algo/bcm_non_local_means_denoising/) with several compuational optimizations expected to applied to gray-scale images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we converted the cropped, resized, and denoised gray-scale image into black and white image by thresholding. Each pixel of a gray-scale image ranges from 0 to 255. We converted the pixels with 200+ pixel value as black and white for others. We chose 200 to capture the pixels that were most definitely grapheme (even after denoising, we attempted to remove any margin of graphemes with smudge/light-gray gradient). (https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_thresholding/py_thresholding.html)\n",
    "\n",
    "Below is the code we used to perform resizing, denoising, and thresholding.\n",
    "\n",
    "```python\n",
    "num_rows = int(np.median(rows)) # 106\n",
    "num_cols = int(np.median(cols)) # 87\n",
    "\n",
    "for i in range(len(train_images)):\n",
    "    images[i] = images[i].resize((num_rows, num_cols), Image.ANTIALIAS)\n",
    "    images[i] = np.array(images[i])\n",
    "    images[i] = cv2.fastNlMeansDenoising(images[i], h=3)\n",
    "    images[i] = cv2.threshold(images[i], 200, 1, cv2.THRESH_BINARY)[1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are the images before and after preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Bottom 10 Grapheme Roots (Before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../figures/bottom_grapheme_roots.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Bottom 10 Grapheme Roots (After)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../figures/grapheme_roots_preprocessed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Top 5 Vowels (Before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../figures/vowels.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Top 5 Vowels (After)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../figures/vowels_preprocessed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  All 7 Consonents (Before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../figures/consonant.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  All 7 Consonents (After)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../figures/consonant_preprocessed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have that the images were preprocessed such that the salient regions were properly cropped, maintaining the graphemes while cutting out the white spaces. The margin of the graphemes were eliminated and resulted in clearer fonts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "We believe our preprocessing was satisfactory, although as you can see from above examples, not all images were preprocessed perfectly. We have one more preprocessing to be done, which is augmentation. However, we plan to use the library function provided by Keras. Our next step is to actually build the model with convolutional layers (CNN) using the preprocessed data. Stay put!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "nteract": {
   "version": "0.21.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
